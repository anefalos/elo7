{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis code\n",
    "\n",
    "Using Unigram and Bigram classifier\n",
    "\n",
    "    1. NLTK\n",
    "    2. SKLEARN\n",
    "    \n",
    "The main idea was to use an already developed human language interpreter, like NLTK (Natural Language Toolkit). I've tried this to save some time.\n",
    "In this case, I also thought Bigram classifier would be one of the best things to use, specially in the negation cases. I've tried Unigram and Bigram separately, but the results were not so good. The best results (92% match) I got combining both classifiers.\n",
    "The only change I've made was in the input files. I've merged them into one single file (\"review_all.txt\") and, at some point in the code, I shuffle the positive and negative reviews.\n",
    "I also included in the files one extra information. One variable (called \"sentiment\") that is \"1\" if the review is positive and \"0\" if the review is negative.\n",
    "Since the data is labeled, my first attempt was to use supervised machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10660, 2)\n",
      "the rock is destined to be the 21st century's new  conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\"\n",
      "1 \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd       \n",
    "#opening the text file with the reviews\n",
    "data = pd.read_csv(\"review_all.txt\", header=0, delimiter=\"\\t\", quoting=2) \n",
    "# 10660 movie reviews (5330 positive and 5330 negative)\n",
    "print data.shape # number of lines and \"columns\"(10660, 2) \n",
    "print data[\"review\"][0]       # Check out the review - only first line\n",
    "# in this file, a positive review means a sentiment = 1 and a \n",
    "# negative review means a sentiment = 0\n",
    "print data[\"sentiment\"][0]    # Check out the sentiment - only first line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "review_data = zip(data[\"review\"], data[\"sentiment\"])\n",
    "# just shuffle the data\n",
    "random.shuffle(review_data)\n",
    "# ~75% for training and ~25% for testing\n",
    "train_X, train_y = zip(*review_data[2660:])\n",
    "test_X, test_y = zip(*review_data[:8000])\n",
    "#\n",
    "# now our data set is ready to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92612499999999998"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################################\n",
    "# using NLTK (Natural Language Toolkit)\n",
    "# a library that works with human language data\n",
    "#####################################################\n",
    "#\n",
    "# tokenizer interface - used to divide a string into\n",
    "# substrings by splitting on the specified string\n",
    "# Ex: \n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "# tknzr = TweetTokenizer()\n",
    "# s0 = \"This is a cooool #dummysmiley: :-)\"\n",
    "# tknzr.tokenize(s0)\n",
    "# ['This', 'is', 'a', 'cooool', '#dummysmiley', ':', ':-)']\n",
    "from nltk import word_tokenize\n",
    "#\n",
    "# \"Convert a collection of text documents to a matrix of token counts\" taken from \n",
    "# http://scikit-learn.org\n",
    "# It belongs to scikit-learn utilities to extract numerical features from text content  \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#\n",
    "# Pipeline - useful when there is a fixed sequence of steps in processing the data:\n",
    "# you only have to call fit and predict once on your data to fit a whole \n",
    "# sequence of estimators.\n",
    "from sklearn.pipeline import Pipeline\n",
    "#\n",
    "# LinearSVC - Linear Support Vector Classification\n",
    "# Useful for labeled data, since it is a supervised learning model\n",
    "# It learns from the training examples and builds a model that \n",
    "# assigns new examples to one category or the other\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "unigram_bigram_sap = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(analyzer=\"word\",\n",
    "                                   # using ngram_range = (1, 2) one obtain\n",
    "                                   # the unigram and bigram model\n",
    "                                   ngram_range=(1, 2),\n",
    "                                   tokenizer=word_tokenize,)),\n",
    "    ('classifier', LinearSVC())\n",
    "])\n",
    " \n",
    "# Fit the model\n",
    "unigram_bigram_sap.fit(train_X, train_y) \n",
    "# Apply transforms, and score with the final estimator\n",
    "unigram_bigram_sap.score(test_X, test_y) \n",
    "\n",
    "# Check the feature names\n",
    "# It is not important. Just if you are curious, like me!!!!\n",
    "#print unigram_bigram_sap.named_steps['vectorizer'].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
